{"cells":[{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["import requests\n","import json\n","import time\n","import pprint as pp\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import udf\n","import web3"]},{"cell_type":"code","execution_count":8,"metadata":{"scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+--------------------+--------------------+\n","|                hash|        from_address|          to_address|               input|\n","+--------------------+--------------------+--------------------+--------------------+\n","|0xf1a7312bec04f7b...|0x93f36c4d96ad967...|0xbc4ca0eda7647a8...|0xa22cb4650000000...|\n","|0x38b6668b1b470fa...|0xb7692c097a20f67...|0xbc4ca0eda7647a8...|0x23b872dd0000000...|\n","|0x72a1c46217d0a16...|0xec6a0c08eb2e8c9...|0xbc4ca0eda7647a8...|0x23b872dd0000000...|\n","|0xe95cb89fc5f8904...|0x7071246948ecad9...|0xbc4ca0eda7647a8...|0xa22cb4650000000...|\n","|0x02a1f7fb349ba99...|0x1ae642ad39c87a5...|0xbc4ca0eda7647a8...|0xa22cb4650000000...|\n","|0xc719973c9f0b390...|0x7512c6a03b4c29e...|0xbc4ca0eda7647a8...|0x095ea7b30000000...|\n","|0x9734adb9d226500...|0x504df0b10056b46...|0xbc4ca0eda7647a8...|0x23b872dd0000000...|\n","|0x7a18926703d3e28...|0xff3d22f855b1630...|0xbc4ca0eda7647a8...|0x23b872dd0000000...|\n","|0xf34db3793a92d48...|0x8ab83d869f2bc25...|0xbc4ca0eda7647a8...|0x23b872dd0000000...|\n","|0x86bcc047764d3bc...|0xf35bb18d4bdc5e0...|0xbc4ca0eda7647a8...|0x23b872dd0000000...|\n","|0x147ced014bf5b54...|0x29cd66ecdb114d4...|0xbc4ca0eda7647a8...|0x23b872dd0000000...|\n","|0xc88367fccc50016...|0xa4bfa0cdc6bbe03...|0xbc4ca0eda7647a8...|0x23b872dd0000000...|\n","|0x96dce81bb0afd2d...|0x6ed6517bdface47...|0xbc4ca0eda7647a8...|0xa22cb4650000000...|\n","|0x575ff2aea3a4c04...|0x0f6685390756029...|0xbc4ca0eda7647a8...|0x23b872dd0000000...|\n","|0x34c5f99a3f95033...|0x6b3bacba7a6da1d...|0xbc4ca0eda7647a8...|0x095ea7b30000000...|\n","|0x076347180b23bf0...|0x48c1bee964913d8...|0xbc4ca0eda7647a8...|0xa22cb4650000000...|\n","|0x54e693efe3a5bf1...|0x4c2a6c5d15502c4...|0xbc4ca0eda7647a8...|0xa22cb4650000000...|\n","|0xa1cbb489a30b988...|0x9888c2fe6c97684...|0xbc4ca0eda7647a8...|0x23b872dd0000000...|\n","|0xde4ff834bd1c2d3...|0x6b6ae848f555f70...|0xbc4ca0eda7647a8...|0xa22cb4650000000...|\n","|0xea07ede50d8e133...|0x4022b1af2be5e94...|0xbc4ca0eda7647a8...|0xa22cb4650000000...|\n","+--------------------+--------------------+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["spark.conf.set(\"viewsEnabled\",\"true\")\n","spark.conf.set(\"materializationDataset\",\"<dataset>\")\n","\n","eth_tx = spark.read.format(\"bigquery\")\\\n","    .option('table', 'bigquery-public-data:crypto_ethereum.transactions') \\\n","    .load()\n","\n","eth_tx.createOrReplaceTempView('eth_tx')\n","sql_hash = \"\"\"\n","select hash, from_address, to_address, input from eth_tx\n","where to_address = lower('0xBC4CA0EdA7647A8aB7C2061c2E118A18a936f13D')\n","\"\"\"\n","tx_hash = spark.sql(sql_hash)\n","tx_hash.show()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["[Row(hash='0x4ffcd8d81aa5ec615a7901e474b99f24a965421f99f868b27c642a052bd5555c', from_address='0x8ce2587b25e56f39785f6ffba8fe95997d40ed9e', to_address='0xbc4ca0eda7647a8ab7c2061c2e118a18a936f13d', input='0x23b872dd0000000000000000000000008ce2587b25e56f39785f6ffba8fe95997d40ed9e000000000000000000000000e6f1ead8e3f682f16a90d4961d47846f69cea076000000000000000000000000000000000000000000000000000000000000166d')]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["df.rdd.take(1)\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["import sys\n","from functools import lru_cache\n","from web3 import Web3\n","from web3.auto import w3\n","from web3.contract import Contract\n","from web3._utils.events import get_event_data\n","from web3._utils.abi import exclude_indexed_event_inputs, get_abi_input_names, get_indexed_event_inputs, normalize_event_input_types\n","from web3.exceptions import MismatchedABI, LogTopicError\n","from web3.types import ABIEvent\n","from eth_utils import event_abi_to_log_topic, to_hex\n","from hexbytes import HexBytes\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import udf\n","\n","import json\n","import re\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["\n","def decode_tuple(t, target_field):\n","    output = dict()\n","    for i in range(len(t)):\n","        if isinstance(t[i], (bytes, bytearray)):\n","            output[target_field[i]['name']] = to_hex(t[i])\n","        elif isinstance(t[i], (tuple)):\n","            output[target_field[i]['name']] = decode_tuple(t[i], target_field[i]['components'])\n","        else:\n","            output[target_field[i]['name']] = t[i]\n","    return output\n","\n","\n","def decode_list_tuple(l, target_field):\n","    output = l\n","    for i in range(len(l)):\n","        output[i] = decode_tuple(l[i], target_field)\n","    return output\n","\n","\n","def decode_list(l):\n","    output = l\n","    for i in range(len(l)):\n","        if isinstance(l[i], (bytes, bytearray)):\n","            output[i] = to_hex(l[i])\n","        else:\n","            output[i] = l[i]\n","    return output\n","\n","\n","def convert_to_hex(arg, target_schema):\n","    output = dict()\n","    for k in arg:\n","        if isinstance(arg[k], (bytes, bytearray)):\n","            output[k] = to_hex(arg[k])\n","        elif isinstance(arg[k], (list)) and len(arg[k]) > 0:\n","            target = [a for a in target_schema if 'name' in a and a['name'] == k][0]\n","            if target['type'] == 'tuple[]':\n","                target_field = target['components']\n","                output[k] = decode_list_tuple(arg[k], target_field)\n","            else:\n","                output[k] = decode_list(arg[k])\n","        elif isinstance(arg[k], (tuple)):\n","            target_field = [a['components'] for a in target_schema if 'name' in a and a['name'] == k][0]\n","            output[k] = decode_tuple(arg[k], target_field)\n","        else:\n","            output[k] = arg[k]\n","    return output\n","\n","# @lru_cache(maxsize=None)\n","def _get_contract(address, abi):\n","    \"\"\"\n","    This helps speed up execution of decoding across a large dataset by caching the contract object\n","    It assumes that we are decoding a small set, on the order of thousands, of target smart contracts\n","    \"\"\"\n","    if isinstance(abi, (str)):\n","        abi = json.loads(abi)\n","\n","    contract = w3.eth.contract(address=Web3.toChecksumAddress(address), abi=abi)\n","    return (contract, abi)\n","\n","@udf\n","def decode_tx(address, input_data, abi):\n","    if abi is not None:\n","        try:\n","            (contract, abi) = _get_contract(address, abi)\n","            func_obj, func_params = contract.decode_function_input(input_data)\n","            target_schema = [a['inputs'] for a in abi if 'name' in a and a['name'] == func_obj.fn_name][0]\n","            decoded_func_params = convert_to_hex(func_params, target_schema)\n","            return func_obj.fn_name # json.dumps(decoded_func_params), json.dumps(target_schema))\n","        except:\n","            e = sys.exc_info()[0]\n","            return ('decode error', repr(e), None)\n","    else:\n","        return ('no matching abi', None, None)\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["('[{\"inputs\": [{\"internalType\": \"string\", \"name\": \"name\", \"type\": \"string\"}, '\n"," '{\"internalType\": \"string\", \"name\": \"symbol\", \"type\": \"string\"}, '\n"," '{\"internalType\": \"uint256\", \"name\": \"maxNftSupply\", \"type\": \"uint256\"}, '\n"," '{\"internalType\": \"uint256\", \"name\": \"saleStart\", \"type\": \"uint256\"}], '\n"," '\"stateMutability\": \"nonpayable\", \"type\": \"constructor\"}, {\"anonymous\": '\n"," 'false, \"inputs\": [{\"indexed\": true, \"internalType\": \"address\", \"name\": '\n"," '\"owner\", \"type\": \"address\"}, {\"indexed\": true, \"internalType\": \"address\", '\n"," '\"name\": \"approved\", \"type\": \"address\"}, {\"indexed\": true, \"internalType\": '\n"," '\"uint256\", \"name\": \"tokenId\", \"type\": \"uint256\"}], \"name\": \"Approval\", '\n"," '\"type\": \"event\"}, {\"anonymous\": false, \"inputs\": [{\"indexed\": true, '\n"," '\"internalType\": \"address\", \"name\": \"owner\", \"type\": \"address\"}, {\"indexed\": '\n"," 'true, \"internalType\": \"address\", \"name\": \"operator\", \"type\": \"address\"}, '\n"," '{\"indexed\": false, \"internalType\": \"bool\", \"name\": \"approved\", \"type\": '\n"," '\"bool\"}], \"name\": \"ApprovalForAll\", \"type\": \"event\"}, {\"anonymous\": false, '\n"," '\"inputs\": [{\"indexed\": true, \"internalType\": \"address\", \"name\": '\n"," '\"previousOwner\", \"type\": \"address\"}, {\"indexed\": true, \"internalType\": '\n"," '\"address\", \"name\": \"newOwner\", \"type\": \"address\"}], \"name\": '\n"," '\"OwnershipTransferred\", \"type\": \"event\"}, {\"anonymous\": false, \"inputs\": '\n"," '[{\"indexed\": true, \"internalType\": \"address\", \"name\": \"from\", \"type\": '\n"," '\"address\"}, {\"indexed\": true, \"internalType\": \"address\", \"name\": \"to\", '\n"," '\"type\": \"address\"}, {\"indexed\": true, \"internalType\": \"uint256\", \"name\": '\n"," '\"tokenId\", \"type\": \"uint256\"}], \"name\": \"Transfer\", \"type\": \"event\"}, '\n"," '{\"inputs\": [], \"name\": \"BAYC_PROVENANCE\", \"outputs\": [{\"internalType\": '\n"," '\"string\", \"name\": \"\", \"type\": \"string\"}], \"stateMutability\": \"view\", \"type\": '\n"," '\"function\"}, {\"inputs\": [], \"name\": \"MAX_APES\", \"outputs\": [{\"internalType\": '\n"," '\"uint256\", \"name\": \"\", \"type\": \"uint256\"}], \"stateMutability\": \"view\", '\n"," '\"type\": \"function\"}, {\"inputs\": [], \"name\": \"REVEAL_TIMESTAMP\", \"outputs\": '\n"," '[{\"internalType\": \"uint256\", \"name\": \"\", \"type\": \"uint256\"}], '\n"," '\"stateMutability\": \"view\", \"type\": \"function\"}, {\"inputs\": [], \"name\": '\n"," '\"apePrice\", \"outputs\": [{\"internalType\": \"uint256\", \"name\": \"\", \"type\": '\n"," '\"uint256\"}], \"stateMutability\": \"view\", \"type\": \"function\"}, {\"inputs\": '\n"," '[{\"internalType\": \"address\", \"name\": \"to\", \"type\": \"address\"}, '\n"," '{\"internalType\": \"uint256\", \"name\": \"tokenId\", \"type\": \"uint256\"}], \"name\": '\n"," '\"approve\", \"outputs\": [], \"stateMutability\": \"nonpayable\", \"type\": '\n"," '\"function\"}, {\"inputs\": [{\"internalType\": \"address\", \"name\": \"owner\", '\n"," '\"type\": \"address\"}], \"name\": \"balanceOf\", \"outputs\": [{\"internalType\": '\n"," '\"uint256\", \"name\": \"\", \"type\": \"uint256\"}], \"stateMutability\": \"view\", '\n"," '\"type\": \"function\"}, {\"inputs\": [], \"name\": \"baseURI\", \"outputs\": '\n"," '[{\"internalType\": \"string\", \"name\": \"\", \"type\": \"string\"}], '\n"," '\"stateMutability\": \"view\", \"type\": \"function\"}, {\"inputs\": [], \"name\": '\n"," '\"emergencySetStartingIndexBlock\", \"outputs\": [], \"stateMutability\": '\n"," '\"nonpayable\", \"type\": \"function\"}, {\"inputs\": [], \"name\": \"flipSaleState\", '\n"," '\"outputs\": [], \"stateMutability\": \"nonpayable\", \"type\": \"function\"}, '\n"," '{\"inputs\": [{\"internalType\": \"uint256\", \"name\": \"tokenId\", \"type\": '\n"," '\"uint256\"}], \"name\": \"getApproved\", \"outputs\": [{\"internalType\": \"address\", '\n"," '\"name\": \"\", \"type\": \"address\"}], \"stateMutability\": \"view\", \"type\": '\n"," '\"function\"}, {\"inputs\": [{\"internalType\": \"address\", \"name\": \"owner\", '\n"," '\"type\": \"address\"}, {\"internalType\": \"address\", \"name\": \"operator\", \"type\": '\n"," '\"address\"}], \"name\": \"isApprovedForAll\", \"outputs\": [{\"internalType\": '\n"," '\"bool\", \"name\": \"\", \"type\": \"bool\"}], \"stateMutability\": \"view\", \"type\": '\n"," '\"function\"}, {\"inputs\": [], \"name\": \"maxApePurchase\", \"outputs\": '\n"," '[{\"internalType\": \"uint256\", \"name\": \"\", \"type\": \"uint256\"}], '\n"," '\"stateMutability\": \"view\", \"type\": \"function\"}, {\"inputs\": [{\"internalType\": '\n"," '\"uint256\", \"name\": \"numberOfTokens\", \"type\": \"uint256\"}], \"name\": \"mintApe\", '\n"," '\"outputs\": [], \"stateMutability\": \"payable\", \"type\": \"function\"}, {\"inputs\": '\n"," '[], \"name\": \"name\", \"outputs\": [{\"internalType\": \"string\", \"name\": \"\", '\n"," '\"type\": \"string\"}], \"stateMutability\": \"view\", \"type\": \"function\"}, '\n"," '{\"inputs\": [], \"name\": \"owner\", \"outputs\": [{\"internalType\": \"address\", '\n"," '\"name\": \"\", \"type\": \"address\"}], \"stateMutability\": \"view\", \"type\": '\n"," '\"function\"}, {\"inputs\": [{\"internalType\": \"uint256\", \"name\": \"tokenId\", '\n"," '\"type\": \"uint256\"}], \"name\": \"ownerOf\", \"outputs\": [{\"internalType\": '\n"," '\"address\", \"name\": \"\", \"type\": \"address\"}], \"stateMutability\": \"view\", '\n"," '\"type\": \"function\"}, {\"inputs\": [], \"name\": \"renounceOwnership\", \"outputs\": '\n"," '[], \"stateMutability\": \"nonpayable\", \"type\": \"function\"}, {\"inputs\": [], '\n"," '\"name\": \"reserveApes\", \"outputs\": [], \"stateMutability\": \"nonpayable\", '\n"," '\"type\": \"function\"}, {\"inputs\": [{\"internalType\": \"address\", \"name\": \"from\", '\n"," '\"type\": \"address\"}, {\"internalType\": \"address\", \"name\": \"to\", \"type\": '\n"," '\"address\"}, {\"internalType\": \"uint256\", \"name\": \"tokenId\", \"type\": '\n"," '\"uint256\"}], \"name\": \"safeTransferFrom\", \"outputs\": [], \"stateMutability\": '\n"," '\"nonpayable\", \"type\": \"function\"}, {\"inputs\": [{\"internalType\": \"address\", '\n"," '\"name\": \"from\", \"type\": \"address\"}, {\"internalType\": \"address\", \"name\": '\n"," '\"to\", \"type\": \"address\"}, {\"internalType\": \"uint256\", \"name\": \"tokenId\", '\n"," '\"type\": \"uint256\"}, {\"internalType\": \"bytes\", \"name\": \"_data\", \"type\": '\n"," '\"bytes\"}], \"name\": \"safeTransferFrom\", \"outputs\": [], \"stateMutability\": '\n"," '\"nonpayable\", \"type\": \"function\"}, {\"inputs\": [], \"name\": \"saleIsActive\", '\n"," '\"outputs\": [{\"internalType\": \"bool\", \"name\": \"\", \"type\": \"bool\"}], '\n"," '\"stateMutability\": \"view\", \"type\": \"function\"}, {\"inputs\": [{\"internalType\": '\n"," '\"address\", \"name\": \"operator\", \"type\": \"address\"}, {\"internalType\": \"bool\", '\n"," '\"name\": \"approved\", \"type\": \"bool\"}], \"name\": \"setApprovalForAll\", '\n"," '\"outputs\": [], \"stateMutability\": \"nonpayable\", \"type\": \"function\"}, '\n"," '{\"inputs\": [{\"internalType\": \"string\", \"name\": \"baseURI\", \"type\": '\n"," '\"string\"}], \"name\": \"setBaseURI\", \"outputs\": [], \"stateMutability\": '\n"," '\"nonpayable\", \"type\": \"function\"}, {\"inputs\": [{\"internalType\": \"string\", '\n"," '\"name\": \"provenanceHash\", \"type\": \"string\"}], \"name\": \"setProvenanceHash\", '\n"," '\"outputs\": [], \"stateMutability\": \"nonpayable\", \"type\": \"function\"}, '\n"," '{\"inputs\": [{\"internalType\": \"uint256\", \"name\": \"revealTimeStamp\", \"type\": '\n"," '\"uint256\"}], \"name\": \"setRevealTimestamp\", \"outputs\": [], \"stateMutability\": '\n"," '\"nonpayable\", \"type\": \"function\"}, {\"inputs\": [], \"name\": '\n"," '\"setStartingIndex\", \"outputs\": [], \"stateMutability\": \"nonpayable\", \"type\": '\n"," '\"function\"}, {\"inputs\": [], \"name\": \"startingIndex\", \"outputs\": '\n"," '[{\"internalType\": \"uint256\", \"name\": \"\", \"type\": \"uint256\"}], '\n"," '\"stateMutability\": \"view\", \"type\": \"function\"}, {\"inputs\": [], \"name\": '\n"," '\"startingIndexBlock\", \"outputs\": [{\"internalType\": \"uint256\", \"name\": \"\", '\n"," '\"type\": \"uint256\"}], \"stateMutability\": \"view\", \"type\": \"function\"}, '\n"," '{\"inputs\": [{\"internalType\": \"bytes4\", \"name\": \"interfaceId\", \"type\": '\n"," '\"bytes4\"}], \"name\": \"supportsInterface\", \"outputs\": [{\"internalType\": '\n"," '\"bool\", \"name\": \"\", \"type\": \"bool\"}], \"stateMutability\": \"view\", \"type\": '\n"," '\"function\"}, {\"inputs\": [], \"name\": \"symbol\", \"outputs\": [{\"internalType\": '\n"," '\"string\", \"name\": \"\", \"type\": \"string\"}], \"stateMutability\": \"view\", \"type\": '\n"," '\"function\"}, {\"inputs\": [{\"internalType\": \"uint256\", \"name\": \"index\", '\n"," '\"type\": \"uint256\"}], \"name\": \"tokenByIndex\", \"outputs\": [{\"internalType\": '\n"," '\"uint256\", \"name\": \"\", \"type\": \"uint256\"}], \"stateMutability\": \"view\", '\n"," '\"type\": \"function\"}, {\"inputs\": [{\"internalType\": \"address\", \"name\": '\n"," '\"owner\", \"type\": \"address\"}, {\"internalType\": \"uint256\", \"name\": \"index\", '\n"," '\"type\": \"uint256\"}], \"name\": \"tokenOfOwnerByIndex\", \"outputs\": '\n"," '[{\"internalType\": \"uint256\", \"name\": \"\", \"type\": \"uint256\"}], '\n"," '\"stateMutability\": \"view\", \"type\": \"function\"}, {\"inputs\": [{\"internalType\": '\n"," '\"uint256\", \"name\": \"tokenId\", \"type\": \"uint256\"}], \"name\": \"tokenURI\", '\n"," '\"outputs\": [{\"internalType\": \"string\", \"name\": \"\", \"type\": \"string\"}], '\n"," '\"stateMutability\": \"view\", \"type\": \"function\"}, {\"inputs\": [], \"name\": '\n"," '\"totalSupply\", \"outputs\": [{\"internalType\": \"uint256\", \"name\": \"\", \"type\": '\n"," '\"uint256\"}], \"stateMutability\": \"view\", \"type\": \"function\"}, {\"inputs\": '\n"," '[{\"internalType\": \"address\", \"name\": \"from\", \"type\": \"address\"}, '\n"," '{\"internalType\": \"address\", \"name\": \"to\", \"type\": \"address\"}, '\n"," '{\"internalType\": \"uint256\", \"name\": \"tokenId\", \"type\": \"uint256\"}], \"name\": '\n"," '\"transferFrom\", \"outputs\": [], \"stateMutability\": \"nonpayable\", \"type\": '\n"," '\"function\"}, {\"inputs\": [{\"internalType\": \"address\", \"name\": \"newOwner\", '\n"," '\"type\": \"address\"}], \"name\": \"transferOwnership\", \"outputs\": [], '\n"," '\"stateMutability\": \"nonpayable\", \"type\": \"function\"}, {\"inputs\": [], \"name\": '\n"," '\"withdraw\", \"outputs\": [], \"stateMutability\": \"nonpayable\", \"type\": '\n"," '\"function\"}]')\n"]}],"source":["TX_HASH = '0x56f2ce34e4b20578742ed8ddc9fcbacaec62d477a530dff7ace8da2fe64b1208'\n","OPENSEA_CONTRACT_ADDR = \"0x7be8076f4ea4a4ad08075c2508e481d6c946d12b\"\n","BORED_APE_ADDR = \"0xBC4CA0EdA7647A8aB7C2061c2E118A18a936f13D\"\n","\n","def fetch_abi(contract_addr):\n","    ABI_ENDPOINT = 'https://api.etherscan.io/api?module=contract&action=getabi&address='\n","    response = requests.get('%s%s'%(ABI_ENDPOINT, contract_addr))\n","    response_json = response.json()\n","    abi_json = json.loads(response_json['result'])\n","    return json.dumps(abi_json)\n","\n","opensea_abi = fetch_abi(OPENSEA_CONTRACT_ADDR)\n","time.sleep(5)\n","nft_abi = fetch_abi(BORED_APE_ADDR)\n","pp.pprint(nft_abi)\n","\n"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["def decode(partitionData):\n","    (contract, abi) = _get_contract(BORED_APE_ADDR, nft_abi)\n","    for row in partitionData:\n","        func_obj, func_params = contract.decode_function_input(row['input'])\n","        target_schema = [a['inputs'] for a in abi if 'name' in a and a['name'] == func_obj.fn_name][0]\n","        decoded_func_params = convert_to_hex(func_params, target_schema)\n","        if func_obj.fn_name == 'transferFrom':\n","            from_ = decoded_func_params['from']\n","            to_ = decoded_func_params['to']\n","            id_ = decoded_func_params['tokenId']\n","            yield (func_obj.fn_name, from_, to_, id_) # json.dumps(decoded_func_params), json.dumps(target_schema))\n","\n","df2 = df.rdd.mapPartitions(decode).toDF(['fn_name', 'from', 'to', 'id'])\n"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------+------------------------------------------+------------------------------------------+----+\n","|fn_name     |from                                      |to                                        |id  |\n","+------------+------------------------------------------+------------------------------------------+----+\n","|transferFrom|0x8ce2587B25e56f39785f6fFba8Fe95997d40ed9e|0xe6f1EaD8e3F682F16A90d4961D47846F69CeA076|5741|\n","|transferFrom|0xe6f1EaD8e3F682F16A90d4961D47846F69CeA076|0x8ce2587B25e56f39785f6fFba8Fe95997d40ed9e|8232|\n","|transferFrom|0xA8FC07d1E0BBE45e47249632d5804955bF82787C|0xd3fC6fec4b219c2D74B366FEe6B585df71611533|7060|\n","|transferFrom|0x937a1faB2930345A4Fd11d10567e82d1b9634c64|0xFba8A2Fa05E0ee39b6e3C584A0f7ce397Cc92aF0|3379|\n","|transferFrom|0xF531c7A28a3492390D4C47dBa6775FA76349DcFF|0xD1F6C71350791f2C26dE8C0E2E5f293a83455C52|3349|\n","|transferFrom|0x9aD14abF4986F635D521033beDd8A8ff61801A41|0xcC87Bcf3fd3120C86532426AC694b5c3B9686179|392 |\n","|transferFrom|0x8c241750F13f0f6d2e95De5Fb4A77e13B4fA730A|0xaF7A08E22784885948150AaFD7ED24e6E9Ca897c|301 |\n","|transferFrom|0xF9E13d8FC6c8F74449a9Cee1088822b817526588|0x677420671845F3BB7a2f59a0cc530198e1f596e9|317 |\n","|transferFrom|0xAe31fa1969F2c8188EC939D2eC1547b82F00a13C|0x98e6F794f33c32Dd46B9F5cbB65C85d563C4BD0a|6108|\n","|transferFrom|0xbE781bB5cd6a89E3AFFc6Abe0F5b74A2377db92E|0xE052113bd7D7700d623414a0a4585BCaE754E9d5|2095|\n","|transferFrom|0xC4b57cE643BcFaEff739b71f27b45059Df992398|0x762b34974ECdBcf8D9015125A48F7B4B6cBAA205|1693|\n","|transferFrom|0xF9E13d8FC6c8F74449a9Cee1088822b817526588|0x677420671845F3BB7a2f59a0cc530198e1f596e9|6708|\n","|transferFrom|0xd3Fb3d25598715806BCBF19116F15469f157Fe2b|0x114def7FdfD25Dbd4957EdCb085ce23793ec9227|791 |\n","|transferFrom|0xbE781bB5cd6a89E3AFFc6Abe0F5b74A2377db92E|0xE052113bd7D7700d623414a0a4585BCaE754E9d5|1487|\n","|transferFrom|0xBc5D327d469Ae9521e192B2eEE8490337AB148a7|0xCC3C0239773768d61bEB9Db78297A5f92c84db8F|6850|\n","|transferFrom|0x21447403D268081A58D9B0c9e2517F0561C2b13c|0xF3f2df7f925026D0c58C971D3A3060DDe8BD8cC0|4283|\n","|transferFrom|0x82d2F6d6C1980aBD6Aa68B535ce55FA911D7C64c|0x4c6b83Ca1c59781faB57790a46281bbd93E539FE|5690|\n","|transferFrom|0xbE781bB5cd6a89E3AFFc6Abe0F5b74A2377db92E|0xE052113bd7D7700d623414a0a4585BCaE754E9d5|1085|\n","|transferFrom|0x79Dc3F8Ab4cb561ba15Aa29503a647201a924832|0x41b1926C7c3447390fC97B603dCc985586E8699B|9685|\n","|transferFrom|0x4a33A29Ade766580eFb33bF8c35b7Dd492C47E37|0x537AedBd6c7380392dB6458889cB463F47fc5b0e|5049|\n","+------------+------------------------------------------+------------------------------------------+----+\n","only showing top 20 rows\n","\n"]}],"source":["df2.show(truncate=False)"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[{"ename":"Py4JJavaError","evalue":"An error occurred while calling o1208.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 213 in stage 66.0 failed 4 times, most recent failure: Lost task 213.3 in stage 66.0 (TID 1522, zhuo-test-0116-w-4.us-central1-b.c.unity-ads-dd-ds-dev-prd.internal, executor 75): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-65-dc86f751e2fb>\", line 4, in decode\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 466, in decode_function_input\n    func = self.get_function_by_selector(selector)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 459, in get_function_by_selector\n    return get_function_by_identifier(fns, 'selector')\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 1681, in get_function_by_identifier\n    'Could not find any function with matching {0}'.format(identifier)\nValueError: Could not find any function with matching selector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:457)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:414)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1926)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1914)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2147)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2096)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2085)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2076)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2097)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2141)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:304)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:2835)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3369)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2835)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-65-dc86f751e2fb>\", line 4, in decode\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 466, in decode_function_input\n    func = self.get_function_by_selector(selector)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 459, in get_function_by_selector\n    return get_function_by_identifier(fns, 'selector')\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 1681, in get_function_by_identifier\n    'Could not find any function with matching {0}'.format(identifier)\nValueError: Could not find any function with matching selector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:457)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:414)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)","\u001B[0;32m<ipython-input-79-770c88a94be5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mcount\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    522\u001B[0m         \u001B[0;36m2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    523\u001B[0m         \"\"\"\n\u001B[0;32m--> 524\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    525\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    526\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mignore_unicode_prefix\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1255\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1256\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1257\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1258\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1259\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m     61\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0ms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m                 raise Py4JJavaError(\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m                 raise Py4JError(\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1208.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 213 in stage 66.0 failed 4 times, most recent failure: Lost task 213.3 in stage 66.0 (TID 1522, zhuo-test-0116-w-4.us-central1-b.c.unity-ads-dd-ds-dev-prd.internal, executor 75): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-65-dc86f751e2fb>\", line 4, in decode\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 466, in decode_function_input\n    func = self.get_function_by_selector(selector)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 459, in get_function_by_selector\n    return get_function_by_identifier(fns, 'selector')\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 1681, in get_function_by_identifier\n    'Could not find any function with matching {0}'.format(identifier)\nValueError: Could not find any function with matching selector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:457)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:414)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1926)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1914)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2147)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2096)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2085)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2076)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2097)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2141)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:304)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:2835)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3369)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2835)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-65-dc86f751e2fb>\", line 4, in decode\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 466, in decode_function_input\n    func = self.get_function_by_selector(selector)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 459, in get_function_by_selector\n    return get_function_by_identifier(fns, 'selector')\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 1681, in get_function_by_identifier\n    'Could not find any function with matching {0}'.format(identifier)\nValueError: Could not find any function with matching selector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:457)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:414)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"]}],"source":["df2.count()"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["from graphframes import GraphFrame\n","from graphframes.examples import Graphs\n"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting graphframes\n","  Downloading https://files.pythonhosted.org/packages/0b/27/c7c7e1ced2fe9a905f865dd91faaec2ac8a8e313f511678c8ec92a41a153/graphframes-0.6-py2.py3-none-any.whl\n","Requirement already satisfied: nose in /opt/conda/anaconda/lib/python3.7/site-packages (from graphframes) (1.3.7)\n","Requirement already satisfied: numpy in /opt/conda/anaconda/lib/python3.7/site-packages (from graphframes) (1.17.2)\n","Installing collected packages: graphframes\n","Successfully installed graphframes-0.6\n"]}],"source":["!pip install graphframes"]},{"cell_type":"code","execution_count":77,"metadata":{},"outputs":[],"source":["df_from = df2.select('from')\n","df_to = df2.select('to')\n","# u = df_from.union(df_to)"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"ename":"Py4JJavaError","evalue":"An error occurred while calling o1385.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 158 in stage 64.0 failed 4 times, most recent failure: Lost task 158.3 in stage 64.0 (TID 1107, zhuo-test-0116-w-9.us-central1-b.c.unity-ads-dd-ds-dev-prd.internal, executor 64): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-65-dc86f751e2fb>\", line 4, in decode\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 466, in decode_function_input\n    func = self.get_function_by_selector(selector)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 459, in get_function_by_selector\n    return get_function_by_identifier(fns, 'selector')\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 1681, in get_function_by_identifier\n    'Could not find any function with matching {0}'.format(identifier)\nValueError: Could not find any function with matching selector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:457)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:414)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1926)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1914)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2147)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2096)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2085)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2076)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2097)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2141)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:304)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:2835)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3369)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2835)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-65-dc86f751e2fb>\", line 4, in decode\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 466, in decode_function_input\n    func = self.get_function_by_selector(selector)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 459, in get_function_by_selector\n    return get_function_by_identifier(fns, 'selector')\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 1681, in get_function_by_identifier\n    'Could not find any function with matching {0}'.format(identifier)\nValueError: Could not find any function with matching selector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:457)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:414)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)","\u001B[0;32m<ipython-input-78-fc6efb7dbfd5>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mdf_from\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36mcount\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    522\u001B[0m         \u001B[0;36m2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    523\u001B[0m         \"\"\"\n\u001B[0;32m--> 524\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcount\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    525\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    526\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mignore_unicode_prefix\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1255\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1256\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1257\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1258\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1259\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m     61\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0ms\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m                 raise Py4JJavaError(\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m                 raise Py4JError(\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1385.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 158 in stage 64.0 failed 4 times, most recent failure: Lost task 158.3 in stage 64.0 (TID 1107, zhuo-test-0116-w-9.us-central1-b.c.unity-ads-dd-ds-dev-prd.internal, executor 64): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-65-dc86f751e2fb>\", line 4, in decode\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 466, in decode_function_input\n    func = self.get_function_by_selector(selector)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 459, in get_function_by_selector\n    return get_function_by_identifier(fns, 'selector')\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 1681, in get_function_by_identifier\n    'Could not find any function with matching {0}'.format(identifier)\nValueError: Could not find any function with matching selector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:457)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:414)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1926)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1914)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1913)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2147)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2096)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2085)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2076)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2097)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2116)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2141)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:990)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:304)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:2835)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3369)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2835)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 400, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-65-dc86f751e2fb>\", line 4, in decode\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 466, in decode_function_input\n    func = self.get_function_by_selector(selector)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/eth_utils/decorators.py\", line 18, in _wrapper\n    return self.method(obj, *args, **kwargs)\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 459, in get_function_by_selector\n    return get_function_by_identifier(fns, 'selector')\n  File \"/opt/conda/anaconda/lib/python3.7/site-packages/web3/contract.py\", line 1681, in get_function_by_identifier\n    'Could not find any function with matching {0}'.format(identifier)\nValueError: Could not find any function with matching selector\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:457)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:592)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:575)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:126)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:414)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:417)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"]}],"source":["df_from.count()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_to.count()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":2}